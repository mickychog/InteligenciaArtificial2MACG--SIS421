{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# Mecanismos de Atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:52.733066Z",
     "start_time": "2020-09-04T12:31:52.725066Z"
    },
    "id": "dYF-N3iRDiQL"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s): #transforma letras especiales en normales\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) # elimina espacio en blanco y lo vuelve minuscula\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #transforma un caracter que no es ingles a un espacio en blanco\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Leer el archivo y dividirlo en líneas\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Dividir cada línea en pares y normalizar\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.056055Z",
     "start_time": "2020-09-04T12:31:52.735065Z"
    },
    "id": "D4dI_X2nDiQL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micky\\.conda\\envs\\InteligenciArtificial\\Lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for tatoeba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tatoeba\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2606fe9edf264f9ca228b7f6ba88de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d7c00d292848e6acc7ef7e9faa8525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9caded91e04bffad51fd9d270b3870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752d63b03b664812a3f22f176879fb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micky\\.conda\\envs\\InteligenciArtificial\\Lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for tatoeba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tatoeba\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pairs = read_file('/content/drive/MyDrive/SIS421/SegundoParcial/Datasets/por.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.071561Z",
     "start_time": "2020-09-04T12:31:56.058156Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFNFMVedDiQM",
    "outputId": "ab9531c9-5279-45f6-bbdc-e612f6a2cb81"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\InteligenciArtificial\\Lib\\random.py:374\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq):\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot choose from an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\InteligenciArtificial\\Lib\\site-packages\\datasets\\dataset_dict.py:81\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     77\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     79\u001b[0m ]\n\u001b[0;32m     80\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\""
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.087569Z",
     "start_time": "2020-09-04T12:31:56.074570Z"
    },
    "id": "NvvgHwvDDiQM"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name #el nombre del lenguaje\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2} #mapear palabras a indices\n",
    "        self.word2count = {} #recuento de todas las palabras del corpus\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"} # mapear los indices a palabras\n",
    "        self.n_words = 3  #palabras especiales para el nlp\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '): #Agrega la sentencia y la separa por ''\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index: #verifica si la palabra no existe en el vocabulario\n",
    "            self.word2index[word] = self.n_words  #la agrega\n",
    "            self.word2count[word] = 1 #inicializa la palabra en 1 ya que es la primera vez que se agrega al vocabulario\n",
    "            self.index2word[self.n_words] = word #actualiza el vocabulario\n",
    "            self.n_words += 1 #se aumenta en 1 la cantidad todal del vocabulario\n",
    "        else:\n",
    "            self.word2count[word] += 1 #se actualiza en 1 si ya existe la palabra\n",
    "\n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')] #convierte las sentencias a indices\n",
    "\n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index] #convierte los indices a sentencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CERKpoMSDiQN"
   },
   "source": [
    "Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud máxima definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.103564Z",
     "start_time": "2020-09-04T12:31:56.088570Z"
    },
    "id": "5T67iEbZDiQN"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "port_prefixes = (\n",
    "    # Presente\n",
    "    \"eu sou \", \"sou \",\n",
    "    \"eu estou \", \"estou \",\n",
    "    \"ele é \", \"é \",\n",
    "    \"ele está \", \"está \",\n",
    "    \"ela é \", \"ela está \",\n",
    "    \"ela está \", \"ela está \",\n",
    "    \"você é \", \"você está \",\n",
    "    \"vocês são \", \"vocês estão \",\n",
    "    \"nós somos \", \"somos \",\n",
    "    \"nós estamos \", \"estamos \",\n",
    "    \"eles são \", \"eles estão \",\n",
    "    \"elas são \", \"elas estão \",\n",
    "    \"eu fui \", \"fui \",\n",
    "    \"eu estive \", \"estive \",\n",
    "    \"ele foi \", \"foi \",\n",
    "    \"ele esteve \", \"esteve \",\n",
    "    \"ela foi \", \"ela esteve \",\n",
    "    \"ela esteve \", \"ela esteve \",\n",
    "    \"você foi \", \"você esteve \",\n",
    "    \"vocês foram \", \"vocês estiveram \",\n",
    "    \"nós fomos \", \"fomos \",\n",
    "    \"nós estivemos \", \"estivemos \",\n",
    "    \"eles foram \", \"eles estiveram \",\n",
    "    \"elas foram \", \"elas estiveram \",\n",
    "    \"eu serei \", \"serei \",\n",
    "    \"eu estarei \", \"estarei \",\n",
    "    \"ele será \", \"será \",\n",
    "    \"ele estará \", \"estará \",\n",
    "    \"ela será \", \"ela estará \",\n",
    "    \"ela estará \", \"ela estará \",\n",
    "    \"você será \", \"você estará \",\n",
    "    \"vocês serão \", \"vocês estarão \",\n",
    "    \"nós seremos \", \"seremos \",\n",
    "    \"nós estaremos \", \"estaremos \",\n",
    "    \"eles serão \", \"eles estarão \",\n",
    "    \"elas serão \", \"elas estarão \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPairs(pairs, filters, lang=0):\n",
    "    return [p for p in pairs if p[lang].startswith(filters)] #Itera sobre cada par del vocab,\n",
    "                                                             #y verifica si la palabra empieza\n",
    "                                                             #por ese filtro y se añade a la lista\n",
    "\n",
    "def trimPairs(pairs):\n",
    "    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH] #devuelve pares de palabras menores\n",
    "                                                                                                         #al tamaño maximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.717657Z",
     "start_time": "2020-09-04T12:31:56.104565Z"
    },
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "354aBbMADiQN",
    "outputId": "e3d9fa76-f91a-4b49-e380-6f10faf2ccd0"
   },
   "outputs": [],
   "source": [
    "def prepareData(file, filters=None, reverse=False):\n",
    "\n",
    "    pairs = read_file(file, reverse) #Leemos el dataset\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "\n",
    "    pairs = trimPairs(pairs) # Devolvemos los pares con longitud menor a maxlenght\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse: #Invierte los pares en caso de que el vocab este en un formato especial\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('port')\n",
    "    else: #Instanciamos los diferentes idiomas\n",
    "        input_lang = Lang('port')\n",
    "        output_lang = Lang('eng')\n",
    "\n",
    "    for pair in pairs: #Iteramos por los pares de frases\n",
    "        input_lang.addSentence(pair[0]) #Agregamos las palabras en eng\n",
    "        output_lang.addSentence(pair[1]) #Agregamos las palabras en port\n",
    "\n",
    "        # agregamos el EOS al final de la frase\n",
    "        pair[0] += \" EOS\"\n",
    "        pair[1] += \" EOS\"\n",
    "\n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('/content/drive/MyDrive/SIS421/SegundoParcial/Datasets/por.txt')\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.730653Z",
     "start_time": "2020-09-04T12:31:59.719659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-GCseNSDiQO",
    "outputId": "0f10fb5f-182e-43e6-efe2-7da675bc6033",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_lang.indexesFromSentence('quero ir ao teu sitio preferido .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.754653Z",
     "start_time": "2020-09-04T12:31:59.731654Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZHX1sDPDiQP",
    "outputId": "d7c5d349-8f92-42f2-8f27-58c60ab13961"
   },
   "outputs": [],
   "source": [
    "output_lang.sentenceFromIndex([601, 312, 145, 1491, 9329, 8816, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:00.233655Z",
     "start_time": "2020-09-04T12:31:59.756655Z"
    },
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKupb-yCDiQP",
    "outputId": "bf7cb2cc-a363-4597-eb09-08e7a9f46ca9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_lang, output_lang, pairs, max_length):\n",
    "        self.input_lang = input_lang #Inicia el primer idioma\n",
    "        self.output_lang = output_lang #Inicia el segundo idioma\n",
    "        self.pairs = pairs #Los pares de frases de un idioma a otro\n",
    "        self.max_length = max_length #tamaño maximo de frases\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs) #Obtenemos la longitud\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        # Convierte la frase del idioma de entrada a índices numéricos\n",
    "        inputs = torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long)\n",
    "        # Convertir la frase del idioma de salida a índices numéricos\n",
    "        outputs = torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n",
    "        # Agregamos relleno a las frases hasta la longitud máxima con el token de relleno correspondiente al idioma de entrada y salida\n",
    "        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.input_lang.word2index['PAD']), \\\n",
    "            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.output_lang.word2index['PAD'])\n",
    "\n",
    "\n",
    "# separamos datos en train 80% -test 20%\n",
    "train_size = len(pairs) * 80 // 100\n",
    "train = pairs[:train_size]\n",
    "test = pairs[train_size:]\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(input_lang, output_lang, train, max_length=MAX_LENGTH),\n",
    "    'test': Dataset(input_lang, output_lang, test, max_length=MAX_LENGTH)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.359235Z",
     "start_time": "2020-09-04T12:32:00.234660Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nE4LXfIJDiQQ",
    "outputId": "46d8acf6-6a6c-4f8c-c9ed-28f66a541cdb"
   },
   "outputs": [],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "\n",
    "input_sentence, output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.374232Z",
     "start_time": "2020-09-04T12:32:01.360239Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htjlVok4DiQQ",
    "outputId": "e1e09887-66ea-461c-82ee-4913db6fc84a"
   },
   "outputs": [],
   "source": [
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.405231Z",
     "start_time": "2020-09-04T12:32:01.375236Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDAmNsaODiQQ",
    "outputId": "618f891e-4a0f-4ddf-cf1e-2b024938e6b9"
   },
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False),\n",
    "}\n",
    "\n",
    "inputs, outputs = next(iter(dataloader['train']))\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:13:53.670033Z",
     "start_time": "2020-09-04T08:13:53.652976Z"
    },
    "id": "g8wCKD8WDiQQ"
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDUDCZj9DiQQ"
   },
   "source": [
    "En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La única diferencia es que, además del último estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.421231Z",
     "start_time": "2020-09-04T12:32:01.406231Z"
    },
    "id": "c0TiTocrDiQR"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        # Inicializamos la capa oculta\n",
    "        self.hidden_size = hidden_size\n",
    "        # Capa de embedding para convertir los índices de las palabras en vectores de embedding\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        # Procesamos los input_sizes, con el vector de embedings y el gru con el hidden_size\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        # Convertimos los índices de las palabras en vectores de embedding\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        # Pasar los embeddings a través de la capa GRU para obtener salidas y el último estado oculto\n",
    "        # preparando para el decoder\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.452231Z",
     "start_time": "2020-09-04T12:32:01.422235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xev8DV0DiQR",
    "outputId": "2e62f598-9bb7-4e6d-a5bf-0f420e92bd13"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size=input_lang.n_words)\n",
    "encoder_outputs, encoder_hidden = encoder(torch.randint(0, input_lang.n_words, (64, 10)))\n",
    "\n",
    "# [batch size, seq len, hidden size]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.468231Z",
     "start_time": "2020-09-04T12:32:01.453237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TM1CjrbDiQR",
    "outputId": "16e5376c-e528-40ab-b80c-6924367af988"
   },
   "outputs": [],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL7-HzX_DiQS"
   },
   "source": [
    "### El *decoder* con *attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:43:51.447608Z",
     "start_time": "2020-09-04T13:43:51.433585Z"
    },
    "id": "u-PCSi79DiQS"
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding para convertir los índices de las palabras en vectores de embedding\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Procesamos los input_sizes, con el vector de embedings y el gru con el hidden_size\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "        # Definimos una capa lineal que proyecta los estados ocultos del decoder,\n",
    "        # con el objetivo de predecir la distribución de probabilidad de las palabras\n",
    "        # de salida en el vocabulario de destino.\n",
    "        self.out = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Toma la capa oculta concatenandolo con el embeding, que tendra una salida\n",
    "        # de longitud maxima, que representa los pesos de atencion para cada elemento en la secuencia de entrada.\n",
    "        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n",
    "\n",
    "        # [ vector oculto | contexto ponderado ], resultado-> Un nuevo vector oculto del tamaño hidensize,\n",
    "        # sacando la prediccion final\n",
    "        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_words, hidden, encoder_outputs):\n",
    "        # sacamos los embeddings\n",
    "        embedded = self.embedding(input_words)\n",
    "        # calculamos los pesos de la capa de atención\n",
    "        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n",
    "        # re-escalamos los outputs del encoder con estos pesos\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n",
    "        # aplicamos la capa de atención\n",
    "        output = self.attn_combine(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        # a partir de aquí, como siempre. La diferencia es que la entrada a la RNN\n",
    "        # no es directmanete el embedding sino una combinación del embedding\n",
    "        # y las salidas del encoder re-escaladas\n",
    "        output, hidden = self.gru(output.unsqueeze(1), hidden)\n",
    "        output = self.out(output.squeeze(1))\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.539231Z",
     "start_time": "2020-09-04T12:32:01.484236Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N43DDrldDiQS",
    "outputId": "1ef74739-bedd-4a80-bc42-b260328bbbaf"
   },
   "outputs": [],
   "source": [
    "decoder = AttnDecoder(input_size=output_lang.n_words)\n",
    "decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, output_lang.n_words, (64, 1)), encoder_hidden, encoder_outputs)\n",
    "\n",
    "# [batch size, vocab size]\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.547232Z",
     "start_time": "2020-09-04T12:32:01.541233Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmdHNpuLDiQS",
    "outputId": "c077f088-7429-4b7e-8364-bb70a4199645"
   },
   "outputs": [],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.578233Z",
     "start_time": "2020-09-04T12:32:01.573232Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37AgkGuiDiQT",
    "outputId": "2704508b-1c31-418a-d7c2-23ed154d12ea"
   },
   "outputs": [],
   "source": [
    "# [batch size, max_length]\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBWXnYY2DiQT"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoZIRkzhDiQT"
   },
   "source": [
    "Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generará la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.593231Z",
     "start_time": "2020-09-04T12:32:01.579232Z"
    },
    "code_folding": [
     3
    ],
    "id": "r3J-4V_MDiQT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def fit(encoder, decoder, dataloader, epochs=10):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            input_sentences, output_sentences = batch\n",
    "            bs = input_sentences.shape[0]\n",
    "            loss = 0\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            # obtenemos el último estado oculto del encoder\n",
    "            encoder_outputs, hidden = encoder(input_sentences)\n",
    "            # calculamos las salidas del decoder de manera recurrente\n",
    "            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "            for i in range(output_sentences.shape[1]):\n",
    "                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                loss += criterion(output, output_sentences[:, i].view(bs))\n",
    "                # el siguiente input será la palabra predicha\n",
    "                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "            # optimización\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n",
    "\n",
    "        val_loss = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(dataloader['test'])\n",
    "            for batch in bar:\n",
    "                input_sentences, output_sentences = batch\n",
    "                bs = input_sentences.shape[0]\n",
    "                loss = 0\n",
    "                # obtenemos el último estado oculto del encoder\n",
    "                encoder_outputs, hidden = encoder(input_sentences)\n",
    "                # calculamos las salidas del decoder de manera recurrente\n",
    "                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "                for i in range(output_sentences.shape[1]):\n",
    "                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                    loss += criterion(output, output_sentences[:, i].view(bs))\n",
    "                    # el siguiente input será la palabra predicha\n",
    "                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")\n",
    "\n",
    "     # Guardamos los pesos entrenados del encoder y decoder\n",
    "    torch.save(encoder.state_dict(), 'encoder_weights.pth')\n",
    "    torch.save(decoder.state_dict(), 'decoder_weights.pth')\n",
    "\n",
    "    # Comprimimos los archivos de pesos en un archivo zip\n",
    "    with zipfile.ZipFile('model_weights.zip', 'w') as zipf:\n",
    "        zipf.write('encoder_weights.pth')\n",
    "        zipf.write('decoder_weights.pth')\n",
    "\n",
    "    # Eliminamos los archivos individuales de pesos\n",
    "    os.remove('encoder_weights.pth')\n",
    "    os.remove('decoder_weights.pth')\n",
    "\n",
    "    print(\"Pesos del modelo guardados y comprimidos en 'model_weights.zip'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:58:02.367102Z",
     "start_time": "2020-09-04T12:32:01.595236Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYaC2gliDiQU",
    "outputId": "fda3237f-464c-4d48-d0d9-a2453b214fd6"
   },
   "outputs": [],
   "source": [
    "fit(encoder, decoder, dataloader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v2avUMiDiQU"
   },
   "source": [
    "## Generando traducciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGu_T1ExDiQU"
   },
   "source": [
    "Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del inglés al castellano de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FBIH2LYYEgY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def load_model_weights(encoder, decoder, zip_path, device):\n",
    "    # Descomprimir el archivo zip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "        zipf.extractall()\n",
    "\n",
    "    # Cargar los pesos en los modelos\n",
    "    encoder.load_state_dict(torch.load('encoder_weights.pth', map_location=device))\n",
    "    decoder.load_state_dict(torch.load('decoder_weights.pth', map_location=device))\n",
    "\n",
    "    # Eliminar los archivos individuales de pesos\n",
    "    os.remove('encoder_weights.pth')\n",
    "    os.remove('decoder_weights.pth')\n",
    "\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    print(\"Pesos del modelo cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F5xfAgoX8qY",
    "outputId": "40b2cfc0-9017-4a15-a51a-e7433a3fbcea"
   },
   "outputs": [],
   "source": [
    "load_model_weights(encoder, decoder, '/content/drive/MyDrive/SIS421/SegundoParcial/model_weights.zip', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:31.925887Z",
     "start_time": "2020-09-04T13:12:31.915888Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlsfKWOtDiQU",
    "outputId": "661f65b7-9df7-4426-ea9c-db0e620378ea"
   },
   "outputs": [],
   "source": [
    "input_sentence, output_sentence = dataset['train'][600]\n",
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:32.615887Z",
     "start_time": "2020-09-04T13:12:32.598887Z"
    },
    "code_folding": [],
    "id": "Tp1QZQnFDiQU"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "    # obtenemos el último estado oculto del encoder\n",
    "    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n",
    "    # calculamos las salidas del decoder de manera recurrente\n",
    "    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n",
    "    # iteramos hasta que el decoder nos de el token <eos>\n",
    "    outputs = []\n",
    "    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "    i = 0\n",
    "    while True:\n",
    "        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_attentions[i] = attn_weights.data\n",
    "        i += 1\n",
    "        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n",
    "        outputs.append(decoder_input.cpu().item())\n",
    "        if decoder_input.item() == output_lang.word2index['EOS']:\n",
    "            break\n",
    "    return output_lang.sentenceFromIndex(outputs), decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:32.789887Z",
     "start_time": "2020-09-04T13:12:32.775888Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o12ey84_DiQV",
    "outputId": "1523ba26-610d-4b75-fc2e-d712be80e9b5"
   },
   "outputs": [],
   "source": [
    "output_words, attn = predict(input_sentence)\n",
    "output_words"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
